# ============================================
# Document Analyzer - Configuração
# ============================================
# Analisador de Documentos Jurídicos/Financeiros
# Versão: 1.1.0
# ============================================

# ============================================
# MODO DE OPERAÇÃO
# ============================================
# Controla como o aplicativo se comporta em relação a:
# - Download de modelos do HuggingFace
# - Uso de APIs cloud (embeddings, geração)
# - Comportamento de fallback
#
# Modos disponíveis:
#   offline - 100% local, sem conexão à internet (PADRÃO)
#   online  - Permite downloads e APIs cloud
#   hybrid  - Tenta online, usa cache local se falhar
# ============================================
system:
  mode: "offline"  # "offline" | "online" | "hybrid"
  
  # Configurações para modo ONLINE
  online:
    allow_model_download: true      # Permite baixar modelos do HuggingFace
    check_for_updates: false        # Verifica atualizações de modelos
    use_cloud_embeddings: false     # Usar API de embeddings (OpenAI, etc.)
    use_cloud_generation: false     # Usar API de geração (GPT-4, Claude, etc.)
    connection_timeout: 10          # Timeout em segundos para conexões
  
  # Configurações para modo OFFLINE
  offline:
    models_path: "./models"         # Caminho dos modelos locais
    strict: true                    # Se true, falha se modelo não existir
  
  # Configurações para modo HYBRID
  hybrid:
    prefer: "online"                # "online" ou "offline"
    fallback_enabled: true          # Usa cache local se online falhar
    fallback_timeout: 5             # Segundos antes de usar fallback

# ============================================
# Configurações gerais
# ============================================
app:
  name: "Document Analyzer (Offline)"
  version: "1.1.0-offline"
  language: "pt-BR"
  log_level: "INFO"

# ============================================
# Perfis de Análise Disponíveis
# ============================================
# Defina qual perfil de análise usar:
# - inventory: Escritura Pública de Inventário
# - meeting_minutes: Ata de Reunião de Quotistas
analysis:
  # Perfil ativo (pode ser sobrescrito via CLI)
  active_profile: "inventory"
  
  # Diretório de instruções
  instructions_dir: "./instructions"
  
  # Definição dos perfis
  profiles:
    inventory:
      name: "Análise de Inventário"
      description: "Extrai herdeiros, inventariante, bens BTG e divisão patrimonial"
      instructions_file: "inventory_analysis.txt"
      analyzer_class: "InventoryAnalyzer"
      
    meeting_minutes:
      name: "Ata de Reunião de Quotistas"
      description: "Extrai ativos envolvidos e suas quantidades"
      instructions_file: "meeting_minutes_analysis.txt"
      analyzer_class: "MeetingMinutesAnalyzer"

# Configurações do Tesseract OCR
ocr:
  tesseract_path: "C:\\Program Files\\Tesseract-OCR\\tesseract.exe"
  language: "por"  # Português
  dpi: 300
  config: "--psm 3 --oem 3"

# Configurações de NLP
nlp:
  mode: "local"
  
  local:
    spacy_model: "pt_core_news_lg"
    sentence_transformer: "./models/embeddings/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa"
    similarity_threshold: 0.75
  
  cloud:
    enabled: false

# ============================================
# Configurações RAG
# ============================================
rag:
  enabled: true
  
  # ============================================
  # Chunking otimizado para Q&A
  # ============================================
  # Estratégias disponíveis:
  #   - recursive: Divisão hierárquica por separadores (padrão anterior)
  #   - semantic_sections: Detecta seções lógicas (RECOMENDADO para licenças)
  #   - paragraph: Preserva parágrafos inteiros
  #   - sentence: Agrupa sentenças
  #   - fixed_size: Tamanho fixo com overlap
  #
  # NOTA: chunk_overlap de 50-100 tokens evita perda de contexto entre chunks
  chunking:
    strategy: "semantic_sections"    # NOVO: detecta seções lógicas do documento
    chunk_size: 800                  # Chunks maiores para mais contexto
    chunk_overlap: 100               # 100 tokens de overlap (50-100 recomendado)
    min_chunk_size: 100
  
  # Embeddings OFFLINE
  embeddings:
    local_model: "./models/embeddings/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa"
    cache_enabled: true
    cache_path: "./cache/embeddings"
  
  # Vector Store
  vector_store:
    type: "faiss"
    use_gpu: false
    index_path: "./cache/index"
  
  # ============================================
  # Retrieval otimizado com BM25 + Embeddings
  # ============================================
  # O sistema usa busca híbrida que combina:
  # 1. BM25 (lexical): Bom para termos técnicos, nomes, siglas (GPL, AGPL, etc.)
  # 2. Embeddings semânticos: Bom para significado e contexto
  # 3. RRF (Reciprocal Rank Fusion): Combina os rankings de forma robusta
  retrieval:
    top_k: 10                      # Número de chunks a retornar
    min_score: 0.2                 # Score mínimo de relevância
    use_reranking: true            # Re-ranking com cross-encoder
    use_hybrid_search: true        # Combina BM25 + embeddings (RECOMENDADO)
    use_mmr: true                  # Maximal Marginal Relevance (diversidade)
    mmr_diversity: 0.3             # Peso da diversidade (0.0 a 1.0)
    
    # Pesos da busca híbrida (soma deve ser 1.0)
    bm25_weight: 0.4               # Peso do BM25 (lexical)
    semantic_weight: 0.6           # Peso dos embeddings (semântico)
  
  # ============================================
  # GERACAO (local e cloud)
  # ============================================
  # Por padrao: usa apenas REGEX para extracao (generate_answers: false)
  # Com generate_answers: true E modo online + cloud habilitado:
  #   - LLM complementa regex para dados que regex nao captura
  #   - Regex tem PRIORIDADE para valores numericos (mais preciso)
  #   - LLM captura contexto, referencias, valores por extenso
  generation:
    mode: "local"
    generate_answers: false  # PADRAO: false (usa apenas regex)
    max_tokens: 500
    temperature: 0.1
    
    # ============================================
    # Modelo Padrao (TinyLlama GGUF - RECOMENDADO)
    # ============================================
    # TinyLlama oferece melhor qualidade que GPT-2 Portuguese
    # com consumo de recursos similar
    default_model: "tinyllama"  # tinyllama | phi3-mini | gpt2-portuguese
    
    # ============================================
    # Modelos Disponiveis
    # ============================================
    # max_context_chars: Quantidade maxima de caracteres do documento
    #                    enviados ao modelo. Valores maiores = mais contexto
    #                    = respostas potencialmente melhores, mas deve
    #                    respeitar o limite de tokens do modelo.
    # 
    # Valores recomendados:
    # | Modelo         | max_context_chars | Justificativa                        |
    # |----------------|-------------------|--------------------------------------|
    # | llama3-8b      | 3500-4500         | MELHOR para PT-BR, janela de 8192    |
    # | mistral-7b     | 2500-3500         | Janela grande, modelo robusto        |
    # | tinyllama      | 400-600           | Modelo pequeno, janela de 2048       |
    # | phi3-mini      | 2000-3000         | NAO recomendado para PT-BR           |
    # | gpt2-portuguese| 400-600           | Muito limitado                       |
    #
    models:
      # TinyLlama 1.1B - PADRAO RECOMENDADO
      # Equilibrio entre tamanho e qualidade
      # NOTA: Com 2048 tokens de contexto e ~4 chars/token em PT-BR,
      #       usamos ~1200 chars de contexto + ~500 para prompt = margem segura
      tinyllama:
        type: "gguf"
        path: "./models/generator/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
        context_length: 2048
        max_tokens: 512           # Aumentado para respostas mais completas
        max_context_chars: 500    # Contexto conservador para balancear com resposta
        description: "TinyLlama 1.1B - Bom para Q&A, ~2GB RAM"
        
      # Phi-3 Mini - ALTA QUALIDADE
      # Melhor qualidade, requer mais recursos
      phi3-mini:
        type: "gguf"
        path: "./models/generator/Phi-3-mini-4k-instruct-q4.gguf"
        context_length: 4096
        max_tokens: 1024
        max_context_chars: 2500    # Janela maior, melhor compreensao
        description: "Phi-3 Mini - Excelente qualidade, ~6GB RAM"
        
      # Mistral 7B - ALTA QUALIDADE
      # Para hardware mais potente
      mistral-7b:
        type: "gguf"
        path: "./models/generator/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
        context_length: 4096
        max_tokens: 1024
        max_context_chars: 3000    # Janela grande, modelo robusto
        description: "Mistral 7B - Alta qualidade, ~8GB RAM"
      
      # Llama 3.1 8B - MELHOR QUALIDADE + PORTUGUÊS
      # Excelente suporte multilíngue incluindo português brasileiro
      # Recomendado para perguntas complexas e documentos em português
      llama3-8b:
        type: "gguf"
        path: "./models/generator/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
        context_length: 8192       # Pode usar até 128K, mas 8K é suficiente
        max_tokens: 1024           # Respostas longas e completas
        max_context_chars: 4000    # 8x mais contexto que TinyLlama!
        description: "Llama 3.1 8B - Melhor para português, ~8GB RAM"
        
      # GPT-2 Portuguese - FALLBACK
      # Menor qualidade, mas sempre funciona
      gpt2-portuguese:
        type: "huggingface"
        path: "./models/generator/models--pierreguillou--gpt2-small-portuguese/snapshots/89a916c041b54c8b925e1a3282a5a334684280cb"
        context_length: 1024
        max_tokens: 500
        max_context_chars: 500     # Muito limitado
        description: "GPT-2 Portuguese - Fallback, qualidade limitada"
    
    # ============================================
    # Provedores Cloud (requer modo online)
    # ============================================
    # Para habilitar: system.mode: "online" + system.online.use_cloud_generation: true
    cloud_providers:
      openai:
        api_key_env: "OPENAI_API_KEY"          # Variável de ambiente com a API key
        embedding_model: "text-embedding-3-small"
        generation_model: "gpt-4o-mini"
        max_tokens: 2000
        temperature: 0.1
      anthropic:
        api_key_env: "ANTHROPIC_API_KEY"
        embedding_model: ""                     # Anthropic não tem embeddings
        generation_model: "claude-3-haiku-20240307"
        max_tokens: 2000
        temperature: 0.1
    
    # ============================================
    # Extração via LLM (complementa Regex)
    # ============================================
    # IMPORTANTE: Regex SEMPRE executa primeiro
    # LLM só executa se: enabled: true E modo online E cloud habilitado
    # Merge: regex_priority = regex tem prioridade para valores numéricos
    llm_extraction:
      enabled: false                            # PADRÃO: false
      provider: "openai"                        # openai | anthropic
      merge_strategy: "regex_priority"          # regex_priority | llm_priority | union
      fallback_to_regex: true                   # Se LLM falhar, usa só regex
      
      # Quais tipos de dados o LLM pode extrair
      # Regex é MELHOR para valores estruturados (true = LLM ajuda)
      extract_valores_monetarios: false         # R$ - regex é mais preciso
      extract_quantidades: false                # Números - regex é mais preciso
      extract_nomes_pessoas: true               # LLM entende contexto
      extract_datas_relativas: true             # "mês passado" → LLM entende
      extract_referencias_contextuais: true     # "conforme acima" → LLM entende
      extract_valores_por_extenso: true         # "trinta mil" → LLM converte
    
    # ============================================
    # Sumarização via LLM (opcional)
    # ============================================
    llm_summarization:
      enabled: false                            # PADRÃO: false
      generate_executive_summary: false         # Resumo executivo no relatório
      generate_insights: false                  # Análises e observações

# ============================================
# Cache de OCR
# ============================================
# Armazena texto extraido de PDFs para evitar
# reprocessamento custoso do OCR
ocr_cache:
  enabled: true
  dir: "./cache/ocr"
  max_age_hours: 720  # 30 dias
  
# Validacao de texto
validation:
  min_word_count: 10
  min_sentence_coherence: 0.6
  check_encoding: true
  language_detection: true

# Configurações de busca
search:
  use_semantic_search: true
  use_keyword_search: true
  combine_results: true
  max_results: 50

# Configurações de saída
output:
  default_dir: "./output"
  highlight_colors:
    heirs: [255, 255, 0]           # Amarelo
    administrator: [0, 255, 0]     # Verde
    btg_assets: [0, 191, 255]      # Azul claro
    divisions: [255, 182, 193]     # Rosa
  output_format: "png"

# ============================================
# Termos específicos para inventário
# ============================================
legal_terms:
  document_types:
    - "escritura pública de inventário"
    - "inventário extrajudicial"
    - "adjudicação"
    - "partilha de bens"
  
  heir_keywords:
    - "herdeiro"
    - "herdeira"
    - "sucessor"
    - "meeiro"
    - "cônjuge"
    - "viúvo"
    - "viúva"
    - "filho"
    - "filha"
    - "neto"
    - "neta"
  
  administrator_keywords:
    - "inventariante"
    - "nomeado inventariante"
    - "exercerá as funções de inventariante"
  
  btg_keywords:
    - "BTG"
    - "BTG Pactual"
    - "Banco BTG"
  
  asset_types:
    - "conta corrente"
    - "poupança"
    - "CDB"
    - "fundo de investimento"
    - "ações"
    - "títulos"
    - "aplicações financeiras"
  
  division_keywords:
    - "coube"
    - "tocou"
    - "partilha"
    - "quinhão"
    - "percentual"
    - "fração"
    - "meação"
    - "legítima"

# ============================================
# Termos específicos para Ata de Reunião
# ============================================
meeting_terms:
  document_types:
    - "ata de reunião"
    - "ata de assembleia"
    - "assembleia de quotistas"
    - "assembleia de cotistas"
    - "reunião de quotistas"
  
  asset_keywords:
    - "ações"
    - "ação"
    - "CRA"
    - "CRI"
    - "debênture"
    - "debêntures"
    - "cotas"
    - "cota"
    - "CDB"
    - "LCI"
    - "LCA"
    - "FII"
    - "FIM"
    - "FIC"
    - "ETF"
    - "título"
    - "títulos"
    - "Tesouro"
    - "NTN"
    - "LFT"
    - "LTN"
    - "valor mobiliário"
    - "valores mobiliários"
  
  quantity_keywords:
    - "quantidade"
    - "volume"
    - "número de"
    - "total de"
    - "unidades"
    - "valor nominal"
    - "valor de face"
    - "valor total"
    - "preço unitário"
    - "distribuição"
    - "emissão"
  
  entity_keywords:
    - "fundo"
    - "administrador"
    - "gestor"
    - "custodiante"
    - "CNPJ"
    - "quotista"
    - "cotista"
    - "emissora"
    - "série"
    - "classe"

# ============================================
# Configurações do Módulo Q&A
# ============================================
# Sistema de Perguntas e Respostas sobre documentos.
# Permite fazer perguntas em linguagem natural e
# receber respostas baseadas no conteúdo do PDF.
qa:
  enabled: true
  
  # ============================================
  # Templates de Prompts
  # ============================================
  # Templates são arquivos .txt em instructions/qa_templates/
  # Veja _COMO_CRIAR_TEMPLATES.txt para criar seus próprios
  templates:
    dir: "./instructions/qa_templates"
    default: "sistema_padrao"
    
    # Detecção automática de template pelo conteúdo
    auto_detect:
      enabled: true
      rules:
        - pattern: "licen[cç]a|GPL|MIT|Apache|open.?source|AGPL|LGPL"
          template: "licencas_software"
        - pattern: "contrato|claus|locação|prestação|parte|contratante"
          template: "contratos"
        - pattern: "ata|reunião|assembleia|quotista|delibera"
          template: "atas_reuniao"
        - pattern: "inventário|herdeiro|espólio|partilha|falecido"
          template: "inventario"
  
  # ============================================
  # Configurações de Conversa
  # ============================================
  conversation:
    max_turns: 10                    # Máximo de perguntas no histórico
    memory_type: "sliding_window"    # full | summary | sliding_window
  
  # ============================================
  # Validação de Respostas
  # ============================================
  validation:
    enabled: true
    min_confidence: 0.5              # Confiança mínima (0.0 a 1.0)
    strict_mode: false               # Se true, validação mais rigorosa
  
  # ============================================
  # Cache de Respostas
  # ============================================
  cache:
    enabled: true
    ttl_hours: 24                    # Tempo de vida do cache em horas
    max_entries: 1000                # Número máximo de entradas
    persist: true                    # Salvar cache em disco
    dir: "./cache/qa_responses"
  
  # ============================================
  # Geração de Respostas
  # ============================================
  generation:
    include_page_references: true    # Incluir "Fonte: página X" nas respostas
    generate_answers: true           # Se false, retorna apenas contexto

