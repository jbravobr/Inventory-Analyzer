# Download de Modelos GGUF para Uso Offline

Este documento explica como baixar os modelos GGUF para uso offline em ambientes corporativos.

> **NOTA**: O wheel do `llama-cpp-python` j√° est√° incluso na pasta `wheels/` e ser√° instalado automaticamente durante a instala√ß√£o offline. N√£o √© mais necess√°rio compilar ou baixar separadamente.

> **üè¢ AMBIENTE CORPORATIVO**: O modelo Llama 3.1 8B est√° dispon√≠vel via **GitHub Releases** do reposit√≥rio, compat√≠vel com proxies corporativos que confiam no GitHub. Os scripts de download usam automaticamente essa fonte.

## Tabela de Modelos Dispon√≠veis

| Modelo | Tamanho | RAM | Qualidade PT-BR | Recomenda√ß√£o |
|--------|---------|-----|-----------------|--------------|
| **Llama 3.1 8B** | ~4.7 GB | ~8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente | **MELHOR para Portugu√™s** |
| Mistral 7B | ~4 GB | ~8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê Muito Boa | Alternativa de qualidade |
| TinyLlama 1.1B | ~670 MB | ~2 GB | ‚≠ê‚≠ê‚≠ê Boa | Recursos limitados |
| Phi-3 Mini | ~2.3 GB | ~6 GB | ‚≠ê‚≠ê Limitada | N√£o recomendado para PT-BR |
| GPT-2 Portuguese | ~500 MB | ~1 GB | ‚≠ê B√°sica | Fallback apenas |

---

## 1. Download Autom√°tico via Script

### PowerShell
```powershell
# Baixar todos os modelos
.\scripts\download_models.ps1

# Baixar apenas Llama 3.1 (RECOMENDADO)
.\scripts\download_models.ps1 -Model llama3

# Baixar apenas TinyLlama
.\scripts\download_models.ps1 -Model tinyllama

# Baixar apenas Mistral
.\scripts\download_models.ps1 -Model mistral
```

### Prompt de Comando (CMD)
```batch
REM Baixar todos os modelos
scripts\download_models.cmd all

REM Baixar apenas Llama 3.1 (RECOMENDADO)
scripts\download_models.cmd llama3

REM Baixar apenas TinyLlama
scripts\download_models.cmd tinyllama

REM Baixar apenas Mistral
scripts\download_models.cmd mistral
```

---

## 2. Download do Llama 3.1 8B (MELHOR PARA PORTUGU√äS)

O Llama 3.1 8B possui excelente suporte ao portugu√™s brasileiro e √© recomendado
para obter respostas de alta qualidade.

### Op√ß√£o A: Via GitHub Releases (RECOMENDADO para ambiente corporativo)

O modelo est√° dispon√≠vel no GitHub Releases do reposit√≥rio, dividido em partes
para contornar o limite de 2GB do GitHub. Os scripts baixam e juntam automaticamente.

**PowerShell:**
```powershell
.\scripts\download_llama3_github.ps1
```

**Prompt de Comando (CMD):**
```batch
scripts\download_llama3_github.cmd
```

**Vantagens:**
- ‚úÖ Compat√≠vel com proxies corporativos que confiam no GitHub
- ‚úÖ Baixa em partes (evita timeout em conex√µes lentas)
- ‚úÖ Verifica√ß√£o de integridade SHA256
- ‚úÖ Junta automaticamente as partes

### Op√ß√£o B: Download direto do HuggingFace

Se o proxy permitir acesso ao HuggingFace:

1. Acesse: https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
2. Baixe o arquivo: `Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf` (~4.7 GB)
3. Coloque em: `models/generator/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`

### Op√ß√£o C: Via PowerShell/curl (HuggingFace)

```powershell
# Windows (PowerShell)
Invoke-WebRequest -Uri "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" -OutFile "models/generator/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
```

```bash
# Linux/Mac
wget -P models/generator/ "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
```

---

## 3. Download do TinyLlama (RECURSOS LIMITADOS)

### Op√ß√£o A: Download direto do HuggingFace

1. Acesse: https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF
2. Baixe o arquivo: `tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf` (~670 MB)
3. Coloque em: `models/generator/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf`

### Op√ß√£o B: Via PowerShell/curl

```powershell
# Windows (PowerShell)
Invoke-WebRequest -Uri "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf" -OutFile "models/generator/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
```

```bash
# Linux/Mac
wget -P models/generator/ "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
```

---

## 4. Outros Modelos (Opcionais)

### Mistral 7B (alternativa de qualidade, ~4 GB)

```powershell
# Windows (PowerShell)
Invoke-WebRequest -Uri "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf" -OutFile "models/generator/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
```

```bash
# Linux/Mac
wget -P models/generator/ "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
```

### Phi-3 Mini (N√ÉO recomendado para portugu√™s, ~2.3 GB)

‚ö†Ô∏è **AVISO**: O Phi-3 Mini tem suporte limitado ao portugu√™s brasileiro.
Use apenas se n√£o puder usar Llama 3.1 ou Mistral.

```powershell
# Windows (PowerShell)
Invoke-WebRequest -Uri "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf" -OutFile "models/generator/Phi-3-mini-4k-instruct-q4.gguf"
```

```bash
# Linux/Mac
wget -P models/generator/ "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
```

---

## 5. llama-cpp-python (J√° Incluso)

O wheel pr√©-compilado do `llama-cpp-python` **j√° est√° incluso** na pasta `wheels/` e ser√° instalado automaticamente durante a instala√ß√£o offline.

### Verificar se est√° instalado

```bash
python -c "import llama_cpp; print('Vers√£o:', llama_cpp.__version__)"
```

### Se precisar reinstalar manualmente

```powershell
# PowerShell
.\scripts\install_llama_cpp.ps1
```

```batch
REM Prompt de Comando
scripts\install_llama_cpp.cmd
```

### Atualizar o wheel (se necess√°rio)

Se precisar atualizar para uma vers√£o mais recente:

1. Acesse: https://github.com/abetlen/llama-cpp-python/releases
2. Baixe o wheel para seu sistema:
   - Windows: `llama_cpp_python-*-cp311-cp311-win_amd64.whl`
   - Linux: `llama_cpp_python-*-cp311-cp311-manylinux_*.whl`
3. Substitua o arquivo na pasta `wheels/`

---

## 6. Verificar Instala√ß√£o

Ap√≥s baixar, verifique:

```bash
# Verificar modelos instalados
python run.py models --check

# Testar Q&A com TinyLlama
python run.py qa documento.pdf -q "teste" --model tinyllama

# Testar Q&A com Llama 3.1 (se dispon√≠vel)
python run.py qa documento.pdf -q "teste" --model llama3-8b
```

---

## 7. Estrutura Final

```
models/
‚îî‚îÄ‚îÄ generator/
    ‚îú‚îÄ‚îÄ Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf   # ~4.7 GB (MELHOR para PT-BR)
    ‚îú‚îÄ‚îÄ mistral-7b-instruct-v0.2.Q4_K_M.gguf     # ~4 GB (alternativa)
    ‚îú‚îÄ‚îÄ tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf     # ~670 MB (recursos limitados)
    ‚îú‚îÄ‚îÄ Phi-3-mini-4k-instruct-q4.gguf           # ~2.3 GB (n√£o recomendado)
    ‚îî‚îÄ‚îÄ models--pierreguillou--gpt2-small-portuguese/  # J√° incluso (fallback)

wheels/
‚îî‚îÄ‚îÄ llama_cpp_python-*-win_amd64.whl             # J√° incluso na instala√ß√£o
```

---

## 8. Sem llama-cpp-python?

Se por algum motivo o llama-cpp-python n√£o funcionar, o sistema usar√° automaticamente
o GPT-2 Portuguese como fallback. A qualidade ser√° menor, mas funcionar√°.

```yaml
# config.yaml - for√ßar GPT-2
rag:
  generation:
    default_model: "gpt2-portuguese"
```

---

## 9. Solu√ß√£o de Problemas

### Erro: "llama_cpp module not found"

```bash
# Reinstalar via script
scripts\install_llama_cpp.cmd
```

### Erro: "Model file not found"

Verifique se o modelo GGUF est√° na pasta correta:
```bash
python run.py models --check
```

### Performance lenta

- Use TinyLlama em m√°quinas com pouca RAM (<8 GB)
- Use Llama 3.1 em m√°quinas com boa RAM (‚â•16 GB)
- Feche outros programas durante o processamento
